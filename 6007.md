6007 Project

1. 我们做的是什么：玩俄罗斯方块的RL agent

2. 方法：

   - 状态空间（s) 定义：4个int

     - lines: 总可消除行数

       holes：总空隙数

     - total_bumpiness：总临列高差

     - sum_height：总高

   - 优化方法：DQN

     状态空间和动作空间都不大，直接用Q网络优化

     loss:
     $$
     \omega ^ {x}  =arg  \min   \frac {1}{2N} \sum _ {i=1}^ {n}  [  Q_ {\omega }  (  s_ {i}  ,  a_ {i}  )-(  r_ {2}  +  \gamma   \max Q_ {\omega }  (s',  a^ {n}  ))]
     $$
     更新公式：
     $$
     Q(s,a)(a,a)+ \alpha [r+ \gamma \max Q(s',a')-Q(s,a)
     $$
     

     推理：通过输入当前状态 s<sub>t</sub> 和一个下一时刻状态 s<sub>t+1</sub> ，Q网络会给出对应的 action reward。遍历所有可能的动作后，agent选取奖励最大的一个action让游戏继续进行。

     训练：通过游戏对局的state信息，更新每一步的Q参数权重

